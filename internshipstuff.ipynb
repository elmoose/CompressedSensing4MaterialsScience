{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.       Try to reproduce the results in Suzuki’s paper\n",
    "**a.       Load dataset into a dataframe.**\n",
    "\n",
    "> Use “WGS_Catalyst_Data_4316_Mpct_Descriptors.csv” file\n",
    "\n",
    "**b.       Try to implement some of the algorithms in the paper, compare with published results**\n",
    "\n",
    "> i. May need to refer to other paper: “Knowledge extraction for water gas shift reaction over noble metal                 catalysts from publications in the literature between 2002 and 2012.pdf”\n",
    "\n",
    "> ii. Look at the algorithms: SVM, XGBoost, Random Forest, etc.\n",
    "\n",
    "**c.       Can use a python script or jupyter notebook. Push code to repository.**\n",
    "\n",
    "\n",
    "2. [BONUS] If have time, then can look into the more sophisticated feature engineering techniques. Look at the papers in folder “Feature Engineering with Compressed Sensing”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn \n",
    "import xgboost\n",
    "import statistics\n",
    "\n",
    "import math\n",
    "import random \n",
    "import re\n",
    "import itertools\n",
    "\n",
    "from collections import Counter \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import binom_test\n",
    "from scipy.stats import norm\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import PolynomialFeatures as plf\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict \n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.gaussian_process.kernels import WhiteKernel, ExpSineSquared\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV\n",
    "from sklearn import linear_model\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4295, 102)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./project_data/WGS_Catalyst_Data_4316_Mpct_Descriptors.csv')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4295, 101)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pt</th>\n",
       "      <th>Au</th>\n",
       "      <th>Ru</th>\n",
       "      <th>Rh</th>\n",
       "      <th>Ir</th>\n",
       "      <th>Cu</th>\n",
       "      <th>Pd</th>\n",
       "      <th>IWI</th>\n",
       "      <th>WI</th>\n",
       "      <th>CI</th>\n",
       "      <th>...</th>\n",
       "      <th>Promo_EN_Allen</th>\n",
       "      <th>Promo_Atomic_Num</th>\n",
       "      <th>Promo_T_melt</th>\n",
       "      <th>Promo_VE_Villars</th>\n",
       "      <th>Promo_Atomic_Wt</th>\n",
       "      <th>Promo_Group</th>\n",
       "      <th>Promo_Period</th>\n",
       "      <th>Promo_Density</th>\n",
       "      <th>Promo_T_boiling</th>\n",
       "      <th>CO_Conversion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.631431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4.396856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.874927</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4270</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.339496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4272</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4273</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4274</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4275</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4276</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4277</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4278</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4280</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4281</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4282</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4283</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>96.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4292</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4293</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.864294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4295 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pt        Au        Ru   Rh   Ir   Cu   Pd  IWI  WI  CI  ...  \\\n",
       "0     0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "1     0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "2     0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "3     0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "4     0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "5     0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "6     0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "7     0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "8     0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "9     0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "10    0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "11    0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "12    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "13    0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "14    0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "15    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "16    0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "17    0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "18    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "19    0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "20    0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "21    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "22    0.0  2.631431  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "23    0.0  4.396856  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "24    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "25    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "26    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "27    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "28    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "29    0.0  0.874927  0.000000  0.0  0.0  0.0  0.0    0   0   0  ...   \n",
       "...   ...       ...       ...  ...  ...  ...  ...  ...  ..  ..  ...   \n",
       "4265  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4266  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4267  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4268  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4269  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4270  0.0  0.000000  1.339496  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4271  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4272  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4273  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4274  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4275  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4276  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4277  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4278  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4279  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4280  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4281  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4282  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4283  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4284  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4285  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4286  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4287  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4288  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4289  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4290  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4291  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4292  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4293  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "4294  0.0  0.000000  2.864294  0.0  0.0  0.0  0.0    1   0   0  ...   \n",
       "\n",
       "      Promo_EN_Allen  Promo_Atomic_Num  Promo_T_melt  Promo_VE_Villars  \\\n",
       "0                0.0               0.0           0.0               0.0   \n",
       "1                0.0               0.0           0.0               0.0   \n",
       "2                0.0               0.0           0.0               0.0   \n",
       "3                0.0               0.0           0.0               0.0   \n",
       "4                0.0               0.0           0.0               0.0   \n",
       "5                0.0               0.0           0.0               0.0   \n",
       "6                0.0               0.0           0.0               0.0   \n",
       "7                0.0               0.0           0.0               0.0   \n",
       "8                0.0               0.0           0.0               0.0   \n",
       "9                0.0               0.0           0.0               0.0   \n",
       "10               0.0               0.0           0.0               0.0   \n",
       "11               0.0               0.0           0.0               0.0   \n",
       "12               0.0               0.0           0.0               0.0   \n",
       "13               0.0               0.0           0.0               0.0   \n",
       "14               0.0               0.0           0.0               0.0   \n",
       "15               0.0               0.0           0.0               0.0   \n",
       "16               0.0               0.0           0.0               0.0   \n",
       "17               0.0               0.0           0.0               0.0   \n",
       "18               0.0               0.0           0.0               0.0   \n",
       "19               0.0               0.0           0.0               0.0   \n",
       "20               0.0               0.0           0.0               0.0   \n",
       "21               0.0               0.0           0.0               0.0   \n",
       "22               0.0               0.0           0.0               0.0   \n",
       "23               0.0               0.0           0.0               0.0   \n",
       "24               0.0               0.0           0.0               0.0   \n",
       "25               0.0               0.0           0.0               0.0   \n",
       "26               0.0               0.0           0.0               0.0   \n",
       "27               0.0               0.0           0.0               0.0   \n",
       "28               0.0               0.0           0.0               0.0   \n",
       "29               0.0               0.0           0.0               0.0   \n",
       "...              ...               ...           ...               ...   \n",
       "4265             0.0               0.0           0.0               0.0   \n",
       "4266             0.0               0.0           0.0               0.0   \n",
       "4267             0.0               0.0           0.0               0.0   \n",
       "4268             0.0               0.0           0.0               0.0   \n",
       "4269             0.0               0.0           0.0               0.0   \n",
       "4270             0.0               0.0           0.0               0.0   \n",
       "4271             0.0               0.0           0.0               0.0   \n",
       "4272             0.0               0.0           0.0               0.0   \n",
       "4273             0.0               0.0           0.0               0.0   \n",
       "4274             0.0               0.0           0.0               0.0   \n",
       "4275             0.0               0.0           0.0               0.0   \n",
       "4276             0.0               0.0           0.0               0.0   \n",
       "4277             0.0               0.0           0.0               0.0   \n",
       "4278             0.0               0.0           0.0               0.0   \n",
       "4279             0.0               0.0           0.0               0.0   \n",
       "4280             0.0               0.0           0.0               0.0   \n",
       "4281             0.0               0.0           0.0               0.0   \n",
       "4282             0.0               0.0           0.0               0.0   \n",
       "4283             0.0               0.0           0.0               0.0   \n",
       "4284             0.0               0.0           0.0               0.0   \n",
       "4285             0.0               0.0           0.0               0.0   \n",
       "4286             0.0               0.0           0.0               0.0   \n",
       "4287             0.0               0.0           0.0               0.0   \n",
       "4288             0.0               0.0           0.0               0.0   \n",
       "4289             0.0               0.0           0.0               0.0   \n",
       "4290             0.0               0.0           0.0               0.0   \n",
       "4291             0.0               0.0           0.0               0.0   \n",
       "4292             0.0               0.0           0.0               0.0   \n",
       "4293             0.0               0.0           0.0               0.0   \n",
       "4294             0.0               0.0           0.0               0.0   \n",
       "\n",
       "      Promo_Atomic_Wt  Promo_Group  Promo_Period  Promo_Density  \\\n",
       "0                 0.0          0.0           0.0            0.0   \n",
       "1                 0.0          0.0           0.0            0.0   \n",
       "2                 0.0          0.0           0.0            0.0   \n",
       "3                 0.0          0.0           0.0            0.0   \n",
       "4                 0.0          0.0           0.0            0.0   \n",
       "5                 0.0          0.0           0.0            0.0   \n",
       "6                 0.0          0.0           0.0            0.0   \n",
       "7                 0.0          0.0           0.0            0.0   \n",
       "8                 0.0          0.0           0.0            0.0   \n",
       "9                 0.0          0.0           0.0            0.0   \n",
       "10                0.0          0.0           0.0            0.0   \n",
       "11                0.0          0.0           0.0            0.0   \n",
       "12                0.0          0.0           0.0            0.0   \n",
       "13                0.0          0.0           0.0            0.0   \n",
       "14                0.0          0.0           0.0            0.0   \n",
       "15                0.0          0.0           0.0            0.0   \n",
       "16                0.0          0.0           0.0            0.0   \n",
       "17                0.0          0.0           0.0            0.0   \n",
       "18                0.0          0.0           0.0            0.0   \n",
       "19                0.0          0.0           0.0            0.0   \n",
       "20                0.0          0.0           0.0            0.0   \n",
       "21                0.0          0.0           0.0            0.0   \n",
       "22                0.0          0.0           0.0            0.0   \n",
       "23                0.0          0.0           0.0            0.0   \n",
       "24                0.0          0.0           0.0            0.0   \n",
       "25                0.0          0.0           0.0            0.0   \n",
       "26                0.0          0.0           0.0            0.0   \n",
       "27                0.0          0.0           0.0            0.0   \n",
       "28                0.0          0.0           0.0            0.0   \n",
       "29                0.0          0.0           0.0            0.0   \n",
       "...               ...          ...           ...            ...   \n",
       "4265              0.0          0.0           0.0            0.0   \n",
       "4266              0.0          0.0           0.0            0.0   \n",
       "4267              0.0          0.0           0.0            0.0   \n",
       "4268              0.0          0.0           0.0            0.0   \n",
       "4269              0.0          0.0           0.0            0.0   \n",
       "4270              0.0          0.0           0.0            0.0   \n",
       "4271              0.0          0.0           0.0            0.0   \n",
       "4272              0.0          0.0           0.0            0.0   \n",
       "4273              0.0          0.0           0.0            0.0   \n",
       "4274              0.0          0.0           0.0            0.0   \n",
       "4275              0.0          0.0           0.0            0.0   \n",
       "4276              0.0          0.0           0.0            0.0   \n",
       "4277              0.0          0.0           0.0            0.0   \n",
       "4278              0.0          0.0           0.0            0.0   \n",
       "4279              0.0          0.0           0.0            0.0   \n",
       "4280              0.0          0.0           0.0            0.0   \n",
       "4281              0.0          0.0           0.0            0.0   \n",
       "4282              0.0          0.0           0.0            0.0   \n",
       "4283              0.0          0.0           0.0            0.0   \n",
       "4284              0.0          0.0           0.0            0.0   \n",
       "4285              0.0          0.0           0.0            0.0   \n",
       "4286              0.0          0.0           0.0            0.0   \n",
       "4287              0.0          0.0           0.0            0.0   \n",
       "4288              0.0          0.0           0.0            0.0   \n",
       "4289              0.0          0.0           0.0            0.0   \n",
       "4290              0.0          0.0           0.0            0.0   \n",
       "4291              0.0          0.0           0.0            0.0   \n",
       "4292              0.0          0.0           0.0            0.0   \n",
       "4293              0.0          0.0           0.0            0.0   \n",
       "4294              0.0          0.0           0.0            0.0   \n",
       "\n",
       "      Promo_T_boiling  CO_Conversion  \n",
       "0                 0.0           7.60  \n",
       "1                 0.0          11.10  \n",
       "2                 0.0          18.00  \n",
       "3                 0.0          13.20  \n",
       "4                 0.0          26.30  \n",
       "5                 0.0          32.10  \n",
       "6                 0.0          25.60  \n",
       "7                 0.0          49.40  \n",
       "8                 0.0          53.50  \n",
       "9                 0.0          58.90  \n",
       "10                0.0          69.10  \n",
       "11                0.0          76.40  \n",
       "12                0.0          81.00  \n",
       "13                0.0          87.50  \n",
       "14                0.0          89.70  \n",
       "15                0.0          90.00  \n",
       "16                0.0          93.20  \n",
       "17                0.0          94.90  \n",
       "18                0.0          92.00  \n",
       "19                0.0          93.30  \n",
       "20                0.0          95.80  \n",
       "21                0.0          93.10  \n",
       "22                0.0          93.30  \n",
       "23                0.0          95.80  \n",
       "24                0.0          57.70  \n",
       "25                0.0          40.20  \n",
       "26                0.0          30.10  \n",
       "27                0.0          74.60  \n",
       "28                0.0          64.60  \n",
       "29                0.0          54.70  \n",
       "...               ...            ...  \n",
       "4265              0.0         100.00  \n",
       "4266              0.0         100.00  \n",
       "4267              0.0         100.00  \n",
       "4268              0.0         100.00  \n",
       "4269              0.0         100.00  \n",
       "4270              0.0         100.00  \n",
       "4271              0.0           1.76  \n",
       "4272              0.0           1.17  \n",
       "4273              0.0           3.13  \n",
       "4274              0.0           9.00  \n",
       "4275              0.0          14.10  \n",
       "4276              0.0          18.80  \n",
       "4277              0.0          24.90  \n",
       "4278              0.0          30.50  \n",
       "4279              0.0          36.40  \n",
       "4280              0.0          46.20  \n",
       "4281              0.0          56.80  \n",
       "4282              0.0          62.80  \n",
       "4283              0.0          69.70  \n",
       "4284              0.0          76.30  \n",
       "4285              0.0          83.60  \n",
       "4286              0.0          89.80  \n",
       "4287              0.0          94.70  \n",
       "4288              0.0          96.30  \n",
       "4289              0.0          95.70  \n",
       "4290              0.0          94.10  \n",
       "4291              0.0          92.40  \n",
       "4292              0.0          90.80  \n",
       "4293              0.0          87.30  \n",
       "4294              0.0          84.30  \n",
       "\n",
       "[4295 rows x 101 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(['Total # of Data'], axis=1)\n",
    "print(data.shape)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.53715979,  0.30288475, -0.0981937 , ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586],\n",
       "       [-0.53715979,  1.62166146, -0.0981937 , ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586],\n",
       "       [-0.53715979,  2.94713638, -0.0981937 , ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586],\n",
       "       ...,\n",
       "       [-0.53715979, -0.35400763,  3.25413083, ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586],\n",
       "       [-0.53715979, -0.35400763,  3.25413083, ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586],\n",
       "       [-0.53715979, -0.35400763,  3.25413083, ..., -0.77796959,\n",
       "        -0.67619778, -0.73143586]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "X=scaler.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Distribution plot\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_color_codes(palette='dark')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "sns.distplot(data['CO_Conversion'], color=\"m\", axlabel='CO Conversion')\n",
    "ax.set(title=\"Histogram for CO_Conversion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Heatmap\n",
    "k = 10\n",
    "corr_mat = data.corr()\n",
    "\n",
    "cols = corr_mat.nlargest(k, 'CO_Conversion')['CO_Conversion'].index\n",
    "cm = np.corrcoef(data[cols].values.T)\n",
    "\n",
    "cmap_ch = sns.cubehelix_palette(as_cmap=True, light=.95)\n",
    "fig, ax = plt.subplots(figsize=(10,10)) \n",
    "hm = sns.heatmap(cm,\n",
    "                 cmap = cmap_ch,\n",
    "                 cbar = True, \n",
    "                 annot = True, # correlation coeff\n",
    "                 square = True, \n",
    "                 robust = True,\n",
    "                 cbar_kws={'fraction' : 0.01}, # shrink colour bar\n",
    "                 annot_kws={'size': 8}, # setting label size\n",
    "                 yticklabels=cols.values, # set y labels\n",
    "                 xticklabels=cols.values,\n",
    "                 linewidth=1,) # Set xlabels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4295, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0         7.60\n",
       "1        11.10\n",
       "2        18.00\n",
       "3        13.20\n",
       "4        26.30\n",
       "5        32.10\n",
       "6        25.60\n",
       "7        49.40\n",
       "8        53.50\n",
       "9        58.90\n",
       "10       69.10\n",
       "11       76.40\n",
       "12       81.00\n",
       "13       87.50\n",
       "14       89.70\n",
       "15       90.00\n",
       "16       93.20\n",
       "17       94.90\n",
       "18       92.00\n",
       "19       93.30\n",
       "20       95.80\n",
       "21       93.10\n",
       "22       93.30\n",
       "23       95.80\n",
       "24       57.70\n",
       "25       40.20\n",
       "26       30.10\n",
       "27       74.60\n",
       "28       64.60\n",
       "29       54.70\n",
       "         ...  \n",
       "4265    100.00\n",
       "4266    100.00\n",
       "4267    100.00\n",
       "4268    100.00\n",
       "4269    100.00\n",
       "4270    100.00\n",
       "4271      1.76\n",
       "4272      1.17\n",
       "4273      3.13\n",
       "4274      9.00\n",
       "4275     14.10\n",
       "4276     18.80\n",
       "4277     24.90\n",
       "4278     30.50\n",
       "4279     36.40\n",
       "4280     46.20\n",
       "4281     56.80\n",
       "4282     62.80\n",
       "4283     69.70\n",
       "4284     76.30\n",
       "4285     83.60\n",
       "4286     89.80\n",
       "4287     94.70\n",
       "4288     96.30\n",
       "4289     95.70\n",
       "4290     94.10\n",
       "4291     92.40\n",
       "4292     90.80\n",
       "4293     87.30\n",
       "4294     84.30\n",
       "Name: CO_Conversion, Length: 4295, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data into features and target\n",
    "X = data.drop(['CO_Conversion'], axis=1)\n",
    "y = data.CO_Conversion\n",
    "print(X.shape)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3006, 100)\n",
      "(1289, 100)\n",
      "(3006,)\n",
      "(1289,)\n"
     ]
    }
   ],
   "source": [
    "#Split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(X)\n",
    "pca.get_covariance()\n",
    "explained_variance=pca.explained_variance_ratio_\n",
    "plt.plot(np.array(range(len(explained_variance))) + 1,explained_variance)\n",
    "plt.xlabel('i-th component')\n",
    "plt.ylabel('explained_variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Cumulative explained variance')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deVyU1f4H8M8wwyo7CLigIoIL4JKZkgvuuOQSimlm5lK/LJfUa2kWlZZmZTdLb+l1X65WWprijhllSm6J4oIoyKIMyr7NADPn9wc1EyU+KMwMA5/36+XrOsPMM98594mP5znPOUcmhBAgIiJ6AAtTF0BERLUfw4KIiCQxLIiISBLDgoiIJDEsiIhIksLUBTwsIQTKyrSmLqNWkMtl0Gh4MxvAtvgrtoUe20LP0lJerfebYVgAOTlFpi6jVnB2tmNb/IFtoce20GNb6DVs6FCt9/MyFBERSWJYEBGRJIYFERFJYlgQEZEkhgUREUkyWFgsWLAAwcHBeOqpp+77cyEE3n//fQwYMADDhg1DXFycoUohIqJqMlhYhIWFYe3atZX+PDo6GklJSTh8+DAWL16Md99911ClEBFRNRlsnkWXLl2Qmppa6c+joqIwcuRIyGQydOzYEXl5ecjIyICHh4ehSiKiekwIAVWZFoUlGhSVaKAq1UBdpoW6TIsSjRalGi1KNAKlGi3KNAKl2vLHGm35nzKtFloBQAACAn/d3KE2TvtzsFZgVIdGsKnmZLw/mWxSnlKphJeXl+6xl5cXlEqlZFjIZOUTbQiQyy3YFn9gW+jVl7bQagUyC0ugzFMho0CNjDw17haokV1YgqyiEmQXlSKvuBQ5xaXIV5UhX1Va/sveQGQywx37UTjaWGLk495wdratkeOZLCzut+eSrAqtzRncepydqse20KsrbVFSpsXtPBVu56qQlqtCep4ayvzy/80oUCOjoASa+/z2d7BWwNlWAWdbS7g72MDbyQYO1grYW8vRwEoBOys57KzksLGUw1phARuFBazk5X8Uchms5BawlMtgKbeAwkIGhVwGuUwGhYUMMpkMMhkgQ9V+X5me0J0L1Z3BbbKw8PLyQnp6uu5xeno6L0ER1TOqUg1Sc1S4lV2E5OxipGQXIy1XhdScYtwtKKlweUdhIYOngzW8HK3RqakTPOyt0dDeGh72VnC3t4J7Ayu4NbCCpVw/FFtXgrM2MFlY9O3bF1u3bsXQoUNx4cIFODg4MCyI6ighBNJyVYi/W4jrGQVIuFeIG/cKkZqjqhAI7g2s0NTZBl2aOaOJky2aONugiZMNGjvZwK2BFSzM4l/zdZPBwmLOnDn47bffkJ2djV69emHGjBkoKysDAIwbNw4hISH46aefMGDAANja2mLJkiWGKoWIjEgIgTt5asSl5+Nyej7i0vMRn1GAwhINAMBCBjR1toVfQ3sMbuuJ5q62aO5iB28XW9hZ1cxgLNU8mbjf4EEtptUKZGYWmLqMWoFdbD22hZ6x20IrBK5nFOJsag5ib+fhQloe7hWWAACs5DL4e9ijjYc9/P/44+tmV2N36EjheaFntmMWRGSehBBIzi5GzK0cnEnJwdmUHOSpyq8aNHK0RmdvJ7Rv7ISgxg5o5d6gwhgCmS+GBRFJKi7VICYpG78kZiEmKRvp+WoA5eHQu5UbHm/mjMeaOsPTwdrElZKhMCyI6L5yikrx0417OJ6QidPJOVCXaWFvLUeXZi54oas3ujZ3QdMauoefaj+GBRHp5BSV4ljCPURdu4uzKTnQCKCxozWebt8IvXxd0amJExS8rFQvMSyI6rl8VRl+TLiHI9fu4vStbGgE4O1sgwldvNHfvyH8PRqYyQQ0MiSGBVE9pC7T4sTNTBy8ehcnbmaiRCPQ2MkGz3XxxoDWDeHfkAFBFTEsiOoJIQSuZRRgz8V0HLp6F/nqMrjaWSKsQ2OEtmmIAC8HBgRVimFBVMcVl2pw4EoGdv1+G/F3C2Ell6GPnzuGBXjh8WbOkFswIEgaw4KojkrOLsbO329jb1w6CtQa+DVsgHl9W2FQ24ZwtLE0dXlkZhgWRHWIVghEX7+L9T8n4kRiFuQWMvTzc0d4x8bo0MSRl5nokTEsiOqAMq3AkWsZ2HAqBYlZRXBrYIWXgpvj6fZecLfnRDmqPoYFkRkr02gReVmJjb+lIDVHBV93O3wyuj2ebOrIZTaoRjEsiMxQmVbgwGUl1p5Kxu1cFdp62uPj4e3Qq5UbXF0acPE8qnEMCyIzohUCR6/dxepfbyE5uxhtPOzxr5EB6NHSleMRZFAMCyIzIITAyaRsrPo5EfF3C9HKvQE+Ht4OIa3cGBJkFAwLolou7k4evvg5EWdTctHYyQaLhrRGaBsP7hpHRsWwIKqlkrOL8eUviTgafw8utpaY19cXT7dvxIFrMgmGBVEtk1NcirUnb2HnhTuwksvwYnAzjH+8KRpY8T9XMh3Js+/evXv49NNPkZGRgbVr1yIhIQHnz59HeHi4MeojqjdKNVp8+/ttrDuVjAJ1GUYEeeGlJ1vAvYGVqUsjgmR/dv78+ejRowcyMjIAAC1atMDmzZsNXhhRffLLzUyM3XQW/z5+E+28HLDt+c54c4A/g4JqDcmwyM7OxpAhQ2BhUf5ShUKh+zsRVU9SVhFmfXcRs7+PgwzAZ2GB+GJUEFq5NzB1aUQVSF6GsrOzQ3Z2tu72vN9//x0ODg4GL4yoLitQl2HdqWRsP5cGG4UFZvduifCOjTl4TbWWZFjMnz8f06ZNQ3JyMsaOHYvs7GysWLHCGLUR1TlCCBy4koHPoxORVViC4YFeeKVnC7ja8XIT1W6SYREQEICtW7ciMTERQgj4+PjA0pLLGxM9rMTMIiw9eh3nU3MR4OWA5SMDEODFXjqZB8k+77Zt21BUVAQ/Pz/4+/ujqKgI27ZtM0ZtRHWCqlSDL39JxLObz+LGvUK8OcAP65/tyKAgsyIZFt988w0cHR11j52cnPDtt98atCiiuiLmVjbGbT6L9TEpGNimIb6d9Diebt+Is6/J7EhehtJqtRBC6Aa4NRoNSktLDV4YkTnLKirBZ8dv4sCVDDRzscV/woPQpZmLqcsiemSSYdGjRw/MmjUL48aNAwDs2LEDPXv2NHhhROZICIF9cUqs+OkmCks0mNKtGSZ1bQZrBe9yIvMmGRbz5s3Djh07sH37dggh0L17d87eJrqPlOxiLDl6HWeSc9ChsSPeHOiHlm6cL0F1g0wIIUxdxMPQagUyMwtMXUat4Oxsx01u/mDKtijTCvzvTCrWnLwFhYUMM3v5YKQJxyV4XuixLfQaNqzeDRWSPYuzZ89i5cqVuH37NsrKynTjF1FRUdX6YKK6IOFuIRYduoYrygL0buWG1/u1QkPueU11kGRYLFy4EAsWLEBgYCCX+SD6Q6lGi40xKVgfkwxHGwWWPtUW/fzduRER1VmSYeHg4ICQkBBj1EJkFq4q87HoUDyu3y3EoLYemNvHF862nKhKdZtkWHTt2hXLli3DwIEDYWWlX5IgICDAoIUR1TYlZVqsO3ULm35LgYudFZaPDEAvXzdTl0VkFJJhceHCBQDApUuXdM/JZDIuU071Slx6PhYdvIabmUUYGuCJOb1bwtGGvQmqPyTDYsuWLY988OjoaHzwwQfQarUIDw/HSy+9VOHnt2/fxhtvvIH8/HxoNBr861//4iUvqlXUZVqs+fUWtp5JgXsDK3wWFojuPq6mLovI6Kq0T+Px48dx/fp1qNVq3XPTp09/4Hs0Gg0WLVqEDRs2wNPTE6NHj0bfvn3RqlUr3Wu+/PJLDB48GM8++ywSEhLw0ksv4dixY4/4VYhqVuztPCw+dA1JWcUYEeSF10Jawt6aW5tS/SR55kdEREClUiEmJgbh4eE4dOgQgoKCJA8cGxuL5s2bw9vbGwAwdOhQREVFVQgLmUyGgoLyORP5+fnw8PB41O9BVGNUpRp8deIW/nc2FZ4O1vhiVCC6tWBvguo3ybA4f/489u7di2HDhmH69OmYNGkSZsyYIXlgpVIJLy8v3WNPT0/ExsZWeM306dMxZcoUbN26FcXFxdiwYYPkcWWy8ok2BMjlFmyLP9RUW5y9lY0F319EYmYRxnXxxryBreFgY169CZ4XemyLmiP5X4GNjQ0AwNbWFkqlEi4uLkhNTZU88P0mhv/9HvTIyEg8/fTTmDx5Ms6fP4/XX38d+/bte+B8DiHAGZl/4OxUveq2RXGpBv/5JQlfn0tDI0drrBodhCeau0CjKkGOqqQGKzU8nhd6bAs9g8/g7t27N/Ly8jBlyhSEhYVBJpNh9OjRkgf28vJCenq67rFSqfzHZaadO3di7dq1AIBOnTpBrVYjOzsbbm68HZGM52xKDt4/HI/UHBXGdGyMV3v6wM5KbuqyiGoVybB49dVXAQChoaHo06cP1Gp1lfbgDgoKQlJSElJSUuDp6YnIyEgsX768wmsaNWqEkydPIiwsDDdu3IBarYarK68Nk3EUlWiw8udEfPv7bTRxssFXY9qjs7ezqcsiqpUqDYuTJ08iODgYhw8fvu/PBw4c+OADKxSIiIjA1KlTodFoMGrUKPj5+WHFihUIDAxEv379MH/+fLz11lvYuHEjZDIZPvzwQy6XQEZxOjkb7x+Kx508NcY+1gSv9GgBW0v2JogqU2lYnD59GsHBwfjxxx/v+3OpsACAkJCQf8ybmDVrlu7vrVq1wo4dO6paK1G1FZaU4YvoROy6cAfezjZY/UwHdGrqZOqyiGq9SsNi5syZ0Gq16NmzJ4YMGWLMmogMIuZWeW9Cma/Gs52bYFr3FrBhb4KoSh64jKyFhQW2bdtmrFqIDKJAXYYPDsdj+s6LsFZY4L9jO2B2b18GBdFDkBzgfvLJJ7Fu3ToMGTIEtra2uuednTkQSLXfyaQsvH8oHvcKS/B8l6Z4Mbg5Q4LoEUiGxa5duwCgQg+Dmx9RbZevKsNnP93AD5eU8HG1w7Lh7RDYyNHUZRGZLcmw4FpNZG5O3MzCkiPlvYkXnvDG1ODmsFZw4y6i6qjSOgbx8fFISEhASYl+JuvIkSMNVhTRo8hTleLT4zcRGaeEr7sdPh4RgHZe1Zu1SkTlJMNi5cqViImJwY0bNxASEoLo6Gh07tyZYUG1StTVDLy1+xKyi0owuas3pnRrDiv2JohqjOR/TYcOHcKmTZvg7u6OpUuXYs+ePRV6GESmlFtcioj9V/HytnNwsbPExvGdMK2HD4OCqIZJ9iysra1hYWEBhUKBgoICuLm5ISUlxRi1ET3QTwn3sOTIdeSqyjC9ty+e7dgIlnKGBJEhSIZFYGAg8vLyEB4ejrCwMNjZ2aF9+/bGqI3ovnKKS/HJsQQcunoXfg0b4PNRQejq78HVRYkMSCbut5Z4JVJTU1FQUIA2bdoYsqYH0moFMjMLTPb5tUl9XH752PV7WHa0vDcxpWszTOrqDcUfexbUt7aoDNtCj22hZ/AlyqdNm4YhQ4agX79+aNq0abU+jOhRZReV4ONjN3Dk2l209rDHF6OC4O9hb+qyiOoNybCYNGkS9u/fj08//RRBQUEYMmQI+vTpA2tra2PUR4Sj1+7io6gE5KvLMK17CzzfpSkUHJsgMqoqX4bSaDQ4deoUvvnmG/z88884d+6coWu7L16G0qvrXeysohJ8FJWAqPh7aOtpj4hBrdHKvcF9X1vX2+JhsC302BZ6Br8MBQAqlQrHjh3DgQMHEBcXh6effrpaH0r0IEIIHPmjN1FUqsErPVpgQhdvKCy41wmRqUiGxWuvvYbY2Fj06NEDzz77LLp27frAPbKJquNeYQmWHb2O4wmZaOflgIhQf/hW0psgIuORDIuwsDAsX74ccjlX6iTDEULg0NW7+ORYAopLNZjR0wfPPt6UvQmiWkIyLHr16mWMOqgeu1egxodHE/DTjUwENXLA26Gt4eNmZ+qyiOgvqjRmQWQIQggcuJKB5T/egLpMi1khLTHusSaQszdBVOswLMgk7haosfTIdfx8MwvtGzvi7VB/tHBlb4Kotqo0LOLi4h74xoCAgBovhuo+IQT2Xy7vTZRotJjduyWe6cTeBFFtV2lYfPjhhwCAkpISXLp0Ca1btwYAXLt2De3bt8f27duNUyHVGRn5aiw9eh2/3MxCh8aOiBjUGs1cbKXfSEQmV2lYbNmyBQAwe/ZsLFq0SBcW8fHxWL9+vXGqozpBCIHIy0os//EGSjUCc/r44plOjWEhY2+CyFxIjlncvHlTFxQA4O/vjytXrhi0KKo7MvLVWHLkOk4kZqFjE0e8HcreBJE5kgwLX19fLFy4EMOHD4dMJsMPP/wAX19fY9RGZkwIgb1xSvz7OHsTRHWBZFgsXboU27dvx+bNmwEAXbp0wbhx4wxeGJkvZb4aS47E49fEbHRq6oS3B/rDm70JIrNWpZ3yxo4di169eqFly5bGqInMlBACey8p8enxG9BoBeb28cUY9iaI6gTJRZ6ioqIwYsQITJ06FQBw5coVvPzyywYvjMyLMl+NWd9dwuLD8fD3sMf2iZ0x9rEmDAqiOkIyLFatWoWdO3fC0dERANC2bVukpaUZvDAyD0II/HApHWM3ncH51Fz8q48vvhrTHk2dedmJqC6RvAwll8vh4FC9ddCpbvrrnU6dmjohItSfIUFUR0mGhZ+fH/bu3QuNRoOkpCRs2bIFnTp1MkZtVEv9fd4ExyaI6j7Jy1Bvv/02EhISYGVlhTlz5sDe3h4LFy40Rm1UC90rUGPO7ji8dzAerdwbYPvzHJsgqg+qvK1qbcFtVfWMuWXkn/tNfHwsAeoyLV7p0aJWrenE7TP12BZ6bAs9g2+rmpiYiPXr1yMtLQ1lZWW65/+cd0F1X1ZRCZYeKd+9LqiRAyIGteYKsUT1jGRYzJo1C2PHjkV4eDi3U62Hjl67i2VRCSgsKcOMnj4Y/3jTWtObICLjkQwLhUKBZ5991hi1UC2SU1yKj6MScPjaXbT1tMc7g9pzL2yiekyyq9CnTx9s27YNGRkZyMnJ0f2piujoaISGhmLAgAFYs2bNfV+zf/9+DBkyBEOHDsXcuXMfrnoyiJ9vZGLsprM4dv0eXu7eHOvHdWRQENVzkj2L77//HgCwbt063XMymQxRUVEPfJ9Go8GiRYuwYcMGeHp6YvTo0ejbty9atWqle01SUhLWrFmD7du3w8nJCZmZmY/6PagGFKjL8O/jN/DDJSX8GjbAirBAtPawN3VZRFQLSIbFsWPHHunAsbGxaN68Oby9vQEAQ4cORVRUVIWw+OabbzB+/Hg4OTkBANzc3B7ps6j6ziTn4L2D15BRoMYLT3jjxeDmsFJwjIqIylUaFidPnkRwcDAOHz58358PHDjwgQdWKpXw8vLSPfb09ERsbGyF1yQlJQEAxo4dC61Wi+nTp6NXr14PPK5MVn47HAFyuUW120JVqsEnR+Kx6eQttHCzw44Xu6GTt3MNVWg8NdEWdQXbQo9tUXMqDYvTp08jODgYP/74431/LhUW95u+IfvbxC2NRoNbt25hy5YtSE9Px/jx47Fv3z7dOlT3Py543/QfqnsP+RVlPt7Zfw2JWUUY07ExZvTygY2l3Czbl/fT67Et9NgWegabZzFz5kwA5ftZPAovLy+kp6frHiuVSnh4eFR4jaenJzp27AhLS0t4e3vDx8cHSUlJaN++/SN9JlVNmVZgY0wy1p5KhpudJb4YFYhuLVxNXRYR1WKSYxYAcPz4cVy/fh1qtVr33PTp0x/4nqCgICQlJSElJQWenp6IjIzE8uXLK7ymf//+iIyMRFhYGLKyspCUlKQb4yDDuJVVhHcPXsOlO/kIbdMQ8/q2gpOtpanLIqJaTjIsIiIioFKpEBMTg/DwcBw6dAhBQUHSB1YoEBERgalTp0Kj0WDUqFHw8/PDihUrEBgYiH79+qFnz544ceIEhgwZArlcjtdffx0uLi418sWoIiEEvo+9g38fvwkrhQU+GNoGA9t4SL+RiAhVWBtq2LBh2Lt3r+5/CwsLMWPGDKxfv95YNVbAtaH0qno9NquoBIsPxeOXm1no2twZEaGt4eFgbYQKjYfXpvXYFnpsCz2Drw1lY2MDALC1tYVSqYSLiwtSU1Or9aFkPCduZmHRoWsoUJdhTh9fPMOlxInoEUiGRe/evZGXl4cpU6YgLCwMMpkMo0ePNkZtVA2qUg2+iE7EN7/fhl/DBlgV3h6tOAubiB7RQy1RXlJSArVabdKd83gZSq+yLnbC3UK8tf8KbtwrwrjHmuDVnj6wruMT7Hi5QY9toce20DPYZajKJuP9SWqeBRmfEAJfn7+NL6Jvwt5agRVhgXjSh7fEElH1VRoWlU3G+xPDonbJKirBooPxOJGYhR4tXfF2qD9c7axMXRYR1RGVhsWjTsYj4zuZlIV3D5QPYs/r2wrhHRv9Y7Y8EVF1SA5wZ2dnY9WqVTh79ixkMhkee+wxvPrqq5wPUQuUlGnx2fGb2HY2Fb7udhzEJiKDkRz1nDNnDlxcXPD5559jxYoVcHV1xezZs41RGz1AcnYxnvnvKWw7m4rRHRph47OdGBREZDCSPYvc3Fy8+uqrusevvPIKjh49atCiqHJCCOy/nIFlUddhpZDj4+Ht0NvP3dRlEVEdJxkWXbt2RWRkJAYPHgwAOHjwIHr37m3ouug+CtRlWBaVgINXMtCpqRNWjO0I26rf+UxE9Mgk51l06tQJxcXFkMvlAMqXFbe1tS1/s0yGc+fOGb7Kv6iv8ywup+djYeQV3M5V4cXg5pjUtRncXBvwHvI/8H56PbaFHttCz+DLfZw/f75aH0DVI4TA9nNp+CI6EW4NrLB6TAd0bOpk6rKIqJ6RHOD+9ttvKzzWaDRYuXKlwQoivZziUszdHYd/H7+J7j6u2DbhMQYFEZmEZFicOnUKL774IjIyMnDt2jWMGTMGhYWFxqitXruQlovxm8/i1K1s/KuPLz4e0Y77ThCRyUhehlq+fDn279+PYcOGwdbWFsuXL0fnzp2NUVu9pBUC286kYtXPifBytMG6cR3R1tN0a3EREQFV6FkkJSVh8+bNCA0NRZMmTbBnzx4UFxcbo7Z6J09Vftnp8+hE9PZzx9YJjzEoiKhWkOxZvPzyy4iIiMCTTz4JIQQ2bNiA0aNHIzIy0hj11RvXlAV4fe9lZOSrMa+vL8I7NuaSHURUa0jeOltQUAB7e/sKzyUlJaFFixaGrKtSdfHW2R8upmNZ1HU421riw2HtENTYsUrv422BemwLPbaFHttCr7q3zkpehpLL5Vi1ahXeeustAOVBkZiYWK0PpXIlZVosORKPxYfj0b6JE7ZOeKzKQUFEZEySYbFgwQJYWVnh999/BwB4eXnhs88+M3hhdV16ngovfX0B38em4/ku3vhiVBBcuKQ4EdVSkmGRnJyMF198EQpF+fCGjY0NHmJzPbqPc6k5eH7reSRmFmHZ8HaY0csHCguOTxBR7SU5wG1lZQWVSqUbbE1OToaVFf8F/Kj2X1Zi8aF4NHaywZoRAWjhZmfqkoiIJEmGxYwZMzB16lTcuXMHc+fOxfnz57kx0iMQQmDtyWSsOXkLjzV1wkfDOcmOiMyH5N1QQPkGSBcuXIAQAh06dICrq+n2dTbHu6FKNVp8cDgekZczMLSdBxYO9IelXPIKoCTe6aHHttBjW+ixLfQMvpAgALi4uHBZ8keUpyrF6z9cxtmUXLz0ZHNM7daM8yeIyOxUKSzo0aTmFGP295eQlqvCe4NbY0g7T1OXRET0SBgWBhKXno/Z312CRgisHB2Ex5o6m7okIqJHVqUL52fOnMGuXbsAAFlZWUhJSTFoUebu18QsvPz1BdhaWmDduI4MCiIye5JhsXLlSqxduxZr1qwBAJSWlmLevHkGL8xcRcYpMWd3HJq52GLds53QwpW3xhKR+ZMMiyNHjuDLL7/UbaXq6enJ/SwqseV0Ct49eA2PNXXC6mc6wL0B56MQUd0gOWZhaWkJmUymu4OnqIi3of2dEAJfnkjChpgU9PdviPcGt4aVovq3xhIR1RaSYTF48GBEREQgLy8P33zzDXbt2oUxY8YYozazoBUCH0clYOeFOxgZ5IX5/f0g59IdRFTHSIbFlClTcOLECTRo0ACJiYmYOXMmunfvbozaaj2NVmDRoWvYfzkDEx5vihm9fDiHgojqJMmw2LhxIwYNGsSA+JsyrcC7B67i0NW7mNa9BSZ19WZQEFGdJRkWBQUFmDJlCpycnDB06FCEhobC3d3dGLXVWmVagYj9V3Hk2l1M7+mDiU94m7okIiKDkhyFnT59OiIjIxEREYGMjAw899xzeOGFF6p08OjoaISGhmLAgAG6W2/v5+DBg2jdujUuXrxY5cJNpUwr8HbkFRy5dhczezEoiKh+qPItO25ubnB3d4ezszMyMzMlX6/RaLBo0SKsXbsWkZGR2LdvHxISEv7xuoKCAmzZsgUdOnR4uMpNQPPHpaej8fcwK6QlJnRhUBBR/SAZFv/73/8wYcIEvPDCC8jOzsb777+PvXv3Sh44NjYWzZs3h7e3N6ysrDB06FBERUX943UrVqzA1KlTYW1t/WjfwEiEEPjw6HUcunoXr/Zogeceb2rqkoiIjEZyzOL27dt488030bZt24c6sFKphJeXl+6xp6cnYmNjK7zm8uXLSE9PR58+fbB+/foqHVcmK1922JiEEFhy4Cp2X0zHtJCWeK2/v1E/vzJyuYXR26K2YlvosS302BY1p9KwKCgogL29PaZOnQoAyMnJqfBzZ+cHr3d0v20y/nq3kFarxdKlSx96IyUhYPT16deevIWNJ29h7GNNMKlzk1qzPj7X6tdjW+ixLfTYFnoG289i7ty5WL16NcLCwiCTySr88pfJZPe9pPRXXl5eSE9P1z1WKpXw8PDQPS4sLER8fDyef/55AMDdu3cxbdo0fPnllwgKCnrkL1TT9l9WYvWvtzA0wBNzerfk7bFEVC9VGharV68GABw7duyRDhwUFISkpCSkpKTA09MTkZGRWL58ue7nDg4OiImJ0T2eMGECXn/99VoVFOdSc7D4UDwe93bCwgF+DAoiqrckB7gnTpxYpef+TqFQICIiAlOnTsWQIUMwePBg+Pn5YUOZI5EAABVzSURBVMWKFZK9ktogKasI8/ZcRhMnGywb3q5GtkElIjJXlfYs1Go1iouLkZ2djdzcXN1lqIKCAmRkZFTp4CEhIQgJCanw3KxZs+772i1btlS1ZoPLV5VhzveXIJfJ8FlYIBxtLE1dEhGRSVUaFjt27MCmTZuQkZGBsLAwXVjY29tj/PjxRivQ2LRC4O39V3E7T42vwtujqbOtqUsiIjK5SsNi4sSJmDhxIrZs2YIJEyYYsyaTWvPrLZxIzMIb/VqhY1MnU5dDRFQrSM6zmDBhAuLj45GQkICSkhLd8yNHjjRoYaZw/Po9rDuVjOGBnhjVoZGpyyEiqjUkw2LlypWIiYnBjRs3EBISgujoaHTu3LnOhUVSVhHePXgN7bwc8Ho/3vlERPRXkrf4HDp0CJs2bYK7uzuWLl2KPXv2VOhh1AWqUg3m770MS7kFlg1rC2vuckdEVIFkz8La2hoWFhZQKBQoKCiAm5sbUlJSjFGbUQgh8GFUAm7eK8LnowLh5Whj6pKIiGodybAIDAxEXl4ewsPDERYWBjs7O7Rv394YtRnFD5fSERmnxNRuzdCthaupyyEiqpVk4n6LOFUiNTUVBQUFaNOmjSFreiCtViAzs6BGjhWfUYDJ239Hh8aO+HxUkNntnc11b/TYFnpsCz22hZ7B1oaKi4ur9E1xcXEICAio1gebmqpUg7cir8LBWoHFQ9uYXVAQERlTpWHx4YcfVvommUyGzZs3G6QgY/nPL0lIzCrCF6MC4WpnZepyiIhqtUrDojYtv1HTTidnY/u5NIR3bMxxCiKiKpAc4N69e/d9nzfXeRb5qjK8dzAezVxsMbOXj6nLISIyC5JhcfHiRd3f1Wo1Tp48iYCAALMNi09+TMC9AjXWjesIG0u5qcshIjILkmHx9ttvV3icn5+PefPmGawgQ/o1MQv7L2dgSrdmCGjkaOpyiIjMxkNPVbaxscGtW7cMUYtBqUo1WBaVgOYutpjctZmpyyEiMiuSPYuXX35Z93chBBISEjB48GCDFmUIa08l43auCl+NaQ8rLudBRPRQJMNi8uTJur/L5XI0adIEXl5eBi2qpiXcLcTWM6kYFuCJzt7Opi6HiMjsSIbFE088AaB8h7yysjIAQE5ODpydzeOXrlYILDlyHQ7WCswMaWnqcoiIzJJkWHz99ddYsWIFbGxsIJPJIISATCYzi320AeDA5QxcvJOHdwe1hrMtt0clInoUkmGxbt067Nu3D66u5jd5rVSjxZpfk9DW0x5D2nmYuhwiIrMlOdLr7e0NW1vz3Id6z8V03M5T4+XuLbiZERFRNUj2LObOnYuxY8eiQ4cOsLLSr6H01ltvGbSw6lKVarDuVDI6NnFEcAsXU5dDRGTWJMMiIiIC3bp1g7+/PywszOeW029/v417hSX44Kk27FUQEVWTZFgoFAosWLDAGLXUmAJ1GTb9loJuzV3wWFPzuGuLiKg2k+wqdO3aFV9//TUyMjKQk5Oj+1Ob7TiXhlxVGab1aGHqUoiI6gTJnsXevXsBAKtXr9Y9V5tvnVWXafHN+dvo0dIV7byqtzMUERGVkwyLY8eOGaOOGnPoSgayi0vxbOcmpi6FiKjOqFP7WQghsP1cGvwaNsDjXNaDiKjG1Kn9LE4n5yDhXiHeDvXnHVBERDWoTu1nsf1cGlztLBHahrO1iYhqUp3Zz+JWVhF+uZmFUR0awZpLkBMR1ag6s5/FjnNpsJTLMKpDY1OXQkRU59SJ/SxUpRpEXlZiYBsPuDWwkn4DERE9lErD4tatW7h3755uP4s/nTlzBiUlJWjWrPZsTfpbcg6KS7UYzLEKIiKDqPTi/pIlS9CgQYN/PG9tbY0lS5YYtKiHFX0jEw2s5HjM28nUpRAR1UmVhkVaWhratGnzj+eDgoKQlpZWpYNHR0cjNDQUAwYMwJo1a/7x8w0bNmDIkCEYNmwYJk6cWOXj/pVWCPx8IxPBLVxhKefANhGRIVT621WtVlf6JpVKJXlgjUaDRYsWYe3atYiMjMS+ffuQkJBQ4TVt27bFrl27sHfvXoSGhuLjjz9+iNLLxd3JR1ZRKUJauT30e4mIqGoqDYugoCB88803/3j+22+/RUBAgOSBY2Nj0bx5c3h7e8PKygpDhw79x3pS3bp1022s1LFjR6Snpz9s/Yi+kQm5DHjSh3tWEBEZSqUD3G+++SamT5+OvXv36sLh0qVLKC0txcqVKyUPrFQqK9w15enpidjY2Epfv3PnTvTq1UvyuDIZ4Oxsp3t8IikbXVq4oplX/RuvkMstKrRFfca20GNb6LEtak6lYeHu7o4dO3bg1KlTuH79OgAgJCQEwcHBVTqwEOIfz1W2BMeePXtw6dIlbN26tQrHBXJyigAAqTnFuJ5RgGF9fHXP1SfOznb18nvfD9tCj22hx7bQa9iweqtwS86z6NatG7p16/bQB/by8qpwWUmpVMLD45+3tv7666/46quvsHXr1grbtlZF9I1MAEDPlq4PXR8REVWdwW4fCgoKQlJSElJSUlBSUoLIyEj07du3wmsuX76MiIgIfPnll3Bze/gB6ugbmfB1t0NTZ9uaKpuIiO5DsmfxyAdWKBAREYGpU6dCo9Fg1KhR8PPzw4oVKxAYGIh+/frho48+QlFREWbNmgUAaNSoEb766qsqHT+3uBS/p+bi+Se8DfUViIjoDzJxv8GFWkyrFcjMLMCRa3fx5r4rWD+uI4IaO5q6LJPg9Vg9toUe20KPbaFX3TELs53FdvF2HqwVFmjraW/qUoiI6jyzDYtLd/LQztMeCs7aJiIyOLP8TVtSpsXVjAIENKqfl5+IiIzNLMPi+t0ClGoEghpV7xocERFVjVmGxcU7+QCAQPYsiIiMwizD4tKdPHjYW8HDwdrUpRAR1QtmGhb57FUQERmR2YWFRqtFWq4KgRyvICIyGrMLi6ISDQCOVxARGZP5hUWpBnIZOBmPiMiIzC4siks08GtoDxtLualLISKqN8wuLIpKNAjgeAURkVGZXVhohUAQxyuIiIzK7MICAO+EIiIyMrMLC7mFDM1cuNkREZExmV1YNHKyqXQvbyIiMgyzCwtn24fbp5uIiKrP7MKCiIiMj2FBRESSGBZERCSJYUFERJIYFkREJIlhQUREkhgWREQkiWFBRESSZEIIYeoiiIiodmPPgoiIJDEsiIhIEsOCiIgkMSyIiEgSw4KIiCQxLIiISBLDgoiIJJlVWERHRyM0NBQDBgzAmjVrTF2O0dy5cwcTJkzA4MGDMXToUGzatAkAkJOTg0mTJmHgwIGYNGkScnNzTVyp8Wg0GowcORL/93//BwBISUlBeHg4Bg4ciNdeew0lJSUmrtA48vLyMHPmTAwaNAiDBw/G+fPn6+15sXHjRgwdOhRPPfUU5syZA7VaXa/OiwULFiA4OBhPPfWU7rnKzgUhBN5//30MGDAAw4YNQ1xcnOTxzSYsNBoNFi1ahLVr1yIyMhL79u1DQkKCqcsyCrlcjvnz5+PAgQP4+uuv8b///Q8JCQlYs2YNgoODcfjwYQQHB9erAN28eTN8fX11jz/55BO88MILOHz4MBwdHbFz504TVmc8H3zwAXr27ImDBw9iz5498PX1rZfnhVKpxObNm7Fr1y7s27cPGo0GkZGR9eq8CAsLw9q1ays8V9m5EB0djaSkJBw+fBiLFy/Gu+++K3l8swmL2NhYNG/eHN7e3rCyssLQoUMRFRVl6rKMwsPDAwEBAQAAe3t7tGzZEkqlElFRURg5ciQAYOTIkTh69KgpyzSa9PR0HD9+HKNHjwZQ/q+kU6dOITQ0FADw9NNP14tzo6CgAKdPn9a1g5WVFRwdHevteaHRaKBSqVBWVgaVSoWGDRvWq/OiS5cucHJyqvBcZefCn8/LZDJ07NgReXl5yMjIeODxzSYslEolvLy8dI89PT2hVCpNWJFppKam4sqVK+jQoQMyMzPh4eEBoDxQsrKyTFydcSxZsgTz5s2DhUX56ZudnQ1HR0coFAoAgJeXV704N1JSUuDq6ooFCxZg5MiRWLhwIYqKiurleeHp6YnJkyejT58+6NGjB+zt7REQEFAvz4u/quxc+Pvv06q0jdmExf2WsJLJZCaoxHQKCwsxc+ZMvPnmm7C3tzd1OSbx448/wtXVFYGBgQ98XX04N8rKynD58mWMGzcOu3fvhq2tbb245HQ/ubm5iIqKQlRUFH7++WcUFxcjOjr6H6+rD+dFVTzK71OFoYqpaV5eXkhPT9c9ViqVusSsD0pLSzFz5kwMGzYMAwcOBAC4ubkhIyMDHh4eyMjIgKurq4mrNLxz587h2LFjiI6OhlqtRkFBAT744APk5eWhrKwMCoUC6enp9eLc8PLygpeXFzp06AAAGDRoENasWVMvz4tff/0VTZs21X3XgQMH4vz58/XyvPirys6Fv/8+rUrbmE3PIigoCElJSUhJSUFJSQkiIyPRt29fU5dlFEIILFy4EC1btsSkSZN0z/ft2xe7d+8GAOzevRv9+vUzVYlGM3fuXERHR+PYsWP49NNP0a1bNyxfvhxdu3bFoUOHAADff/99vTg3GjZsCC8vL9y8eRMAcPLkSfj6+tbL86Jx48a4cOECiouLIYTAyZMn0apVq3p5XvxVZefCn88LIfD777/DwcFBMizMaonyn376CUuWLIFGo8GoUaMwbdo0U5dkFGfOnMH48ePh7++vu04/Z84ctG/fHq+99hru3LmDRo0aYcWKFXB2djZxtcYTExOD9evXY/Xq1UhJScHs2bORm5uLtm3b4pNPPoGVlZWpSzS4K1euYOHChSgtLYW3tzeWLl0KrVZbL8+Lzz//HPv374dCoUDbtm3xwQcfQKlU1pvzYs6cOfjtt9+QnZ0NNzc3zJgxA/3797/vuSCEwKJFi/Dzzz/D1tYWS5YsQVBQ0AOPb1ZhQUREpmE2l6GIiMh0GBZERCSJYUFERJIYFkREJIlhQUREkhgWZBStW7fGhx9+qHu8bt06fPHFFzVy7Pnz5+PgwYM1cqwHOXDgAAYPHowJEyYY/LNM7auvvjJ1CVTLMCzIKKysrHD48OFat06RRqOp8mt37tyJd955B1u2bDFgRbXD6tWrTV0C1TJms9wHmTeFQoFnnnkGmzZtwuzZsyv8bP78+ejduzcGDRoEAOjUqRPOnz+PmJgYfPHFF3Bzc8PVq1cxYMAA+Pv7Y/PmzVCr1Vi1ahWaNWsGoHy5h82bNyMzMxPz589Hnz59oNFo8Mknn+C3335DSUkJxo8fj7FjxyImJgYrV66Eh4cHrly5gv3791eoZ9++fVi9ejWEEAgJCcG8efOwcuVKnDt3Du+88w769u2LN954o8J7/vvf/+KHH36ATCZDr1698K9//QtXrlzBO++8g+LiYjRr1gxLliyBk5MTJkyYgLZt2yIuLg5ZWVlYtmwZ1qxZg/j4eAwePBizZ89Gamoqpk6dig4dOuDy5cvw8fHBsmXLYGtri5MnT2LZsmXQaDQIDAzEe++9BysrK/Tt2xcjR47Ejz/+iLKyMnz22Wfw9fVFUVERFi9ejPj4eGg0GkyfPh39+/fHd999h2PHjqG4uBgpKSno378/Xn/9dXzyySdQqVQYMWIEWrVqhcWLF+O1115Deno6tFotXnnlFQwZMsSAZwvVSoLICDp27Cjy8/NFnz59RF5enli7dq34/PPPhRBCvPHGG+LAgQMVXiuEEKdOnRKdO3cWSqVSqNVq0aNHD7FixQohhBAbN24U77//vu79kydPFhqNRiQmJoqePXsKlUolduzYIVatWiWEEEKtVounn35aJCcni1OnTokOHTqI5OTkf9SZnp4uQkJCRGZmpigtLRUTJkwQR44cEUII8dxzz4nY2Nh/vOf48ePimWeeEUVFRUIIIbKzs4UQQjz11FMiJiZGCCHEZ599pqv3ueeeEx999JHue3Tv3l33HXv27CmysrJESkqK8Pf3F2fOnBFCCDF//nyxdu1aoVKpRK9evcTNmzeFEELMmzdPbNiwQQghRJ8+fcTmzZuFEEJs3bpVvPnmm0IIIZYvXy52794thBAiNzdXDBw4UBQWFopdu3aJvn37iry8PKFSqUTv3r3F7du3K/x/IIQQBw8eFAsXLtQ9zsvLq+z/ZqrDeBmKjMbe3h4jRozA5s2bq/yeoKAgeHh4wMrKCs2aNUP37t0BAP7+/khLS9O9bvDgwbCwsECLFi3g7e2Nmzdv4sSJE9izZw9GjBiB8PBw5OTk4NatW7rjent7/+PzLl68iCeeeAKurq5QKBQYNmwYTp8+/cAaT548ibCwMNja2gIAnJ2dkZ+fj/z8fDzxxBMAyvdSOHPmjO49f65R5O/vDz8/P9139Pb21i3w1qhRI3Tu3BkAMHz4cJw9exaJiYlo2rQpfHx87nvcPxeZDAwM1LXPL7/8gv/+978YMWIEJkyYALVajTt37gAAgoOD4eDgAGtra/j6+lZo0z/5+/vj119/xccff4wzZ87AwcHhge1BdRMvQ5FRTZw4EWFhYQgLC9M9J5fLodVqAZQvmlhaWqr72V/X8bGwsNA9trCwqDDe8PfllWUyGYQQeOutt9CzZ88KP4uJiYGdnV2NfSchxEMvff3X7/H371hWVgag8u/0IJaWlrrj/LV9Pv/8c7Rs2bLCay9cuFDhs+Vy+X3HcHx8fPDdd9/hp59+wvLly9G9e3dMnz69Kl+T6hD2LMionJ2dMWjQoArbWzZp0kS3B3BUVFSFsKiqgwcPQqvVIjk5GSkpKfDx8UGPHj2wfft23fESExNRVFT0wOO0b98ep0+fRlZWlm5rzi5dujzwPd27d8euXbtQXFwMoHzfYwcHBzg6Our+1b9nzx7J4/zd7du3cf78eQBAZGQkOnfujJYtWyItLU3XQ6rKcXv06IGtW7fqguby5cuSn61QKHTtplQqYWtrixEjRmDKlClVej/VPexZkNFNnjwZ27Zt0z0eM2YMXnnlFYwePRrBwcGP9K9+Hx8fPPfcc8jMzMR7770Ha2trhIeHIy0tDWFhYRBCwMXFBf/5z38eeBwPDw/MmTMHEydOhBACvXr1Qv/+/R/4nl69euHq1asYNWoULC0tERISgjlz5mDZsmW6Ae4/V4R9GL6+vvj+++8RERGBFi1aYNy4cbC2tsbSpUsxa9Ys3QD3uHHjHnicV155BUuWLMHw4cMhhECTJk0k73YaM2YMhg8fjnbt2mHkyJH46KOPYGFhAYVCUaX9mqnu4aqzRLVQamoqXn75Zezbt8/UpRAB4GUoIiKqAvYsiIhIEnsWREQkiWFBRESSGBZERCSJYUFERJIYFkREJOn/ASXHAq1BTt/zAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlim(0,100,1)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.37633129, -4.19646007,  0.60388809, ..., -0.43009051,\n",
       "        -0.10105822,  0.01013673],\n",
       "       [-3.47589303, -4.53881144,  0.74270992, ..., -0.05634831,\n",
       "        -0.02946902,  0.03696978],\n",
       "       [-3.57596045, -4.88290165,  0.88223684, ...,  0.31929217,\n",
       "         0.04248378,  0.06393911],\n",
       "       ...,\n",
       "       [-1.09236709,  7.93186866,  1.51896355, ...,  0.07221339,\n",
       "         0.40421249, -0.00828615],\n",
       "       [-1.07496419,  7.9381226 ,  1.5092836 , ...,  0.0623346 ,\n",
       "         0.40344828, -0.01225406],\n",
       "       [-1.05914337,  7.94380801,  1.50048364, ...,  0.05335388,\n",
       "         0.40275356, -0.01586124]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca=PCA(n_components=80)\n",
    "X2=pca.fit_transform(X)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bd0a9a9ae0a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Split into train and test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#Split into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 598597.9283168401, tolerance: 252.49978601863205\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 581013.9525698809, tolerance: 249.5464769444683\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 610485.3136791267, tolerance: 249.36201912777224\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 618140.2003993746, tolerance: 253.42053915640201\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 593929.4482941999, tolerance: 249.05510193149524\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 592287.0842920841, tolerance: 252.49978601863205\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 575371.3134261932, tolerance: 249.5464769444683\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 599407.2803785076, tolerance: 249.36201912777224\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 609662.9358014389, tolerance: 253.42053915640201\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 582059.1528730294, tolerance: 249.05510193149524\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 536956.9494050537, tolerance: 252.49978601863205\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 525862.4083826805, tolerance: 249.5464769444683\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 505822.1730688776, tolerance: 249.36201912777224\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 538281.3620513472, tolerance: 253.42053915640201\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 484200.1033169159, tolerance: 249.05510193149524\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 103236.94425539754, tolerance: 252.49978601863205\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 123915.38786543289, tolerance: 249.5464769444683\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 32557.98055527429, tolerance: 249.36201912777224\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 97984.47050763376, tolerance: 253.42053915640201\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 60736.04469194752, tolerance: 249.05510193149524\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso best parameters:{'fit__alpha': 0.0001}\n",
      "lasso best score-23.280959799497356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 65898.95388652105, tolerance: 313.4979707694146\n",
      "  positive)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lasso' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d2556ac6d101>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lasso best parameters:\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lasso best score\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlasso_regressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcoef\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lasso' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "lasso_params = {'fit__alpha':[1e-8, 1e-6, 1e-5,1e-4]\n",
    "}\n",
    "lasso_pipe = Pipeline([\n",
    "                 ('fit', linear_model.Lasso(normalize = True))]) \n",
    "lasso_regressor = GridSearchCV(lasso_pipe,lasso_params,scoring=\"neg_root_mean_squared_error\",cv=5)\n",
    "\n",
    "lasso_regressor.fit(X_train,y_train)\n",
    "\n",
    "print(\"lasso best parameters:\"+ str(lasso_regressor.best_params_))\n",
    "print(\"lasso best score\" + str(lasso_regressor.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 666913.0057197793, tolerance: 283.22311868342155\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 688392.3498447562, tolerance: 282.7787935632602\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 662909.0707376724, tolerance: 280.91643030579576\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675038.570362811, tolerance: 281.9867903623543\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691501.2127626139, tolerance: 280.3100533675053\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 676557.1397377818, tolerance: 282.25500851243197\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 687809.0782442635, tolerance: 283.97788431827763\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 688409.680898163, tolerance: 283.3227122371475\n",
      "  positive)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675403.8976741083, tolerance: 281.32803309270656\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-22.39914241751743\n",
      "validation score:-23.17569638969008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675663.1958676655, tolerance: 281.2978754116164\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "best_params = lasso_regressor.best_params_\n",
    "lasso = linear_model.Lasso(alpha=best_params['fit__alpha'])\n",
    "scores = cross_validate(lasso, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:23.175437534786028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756769.3904974766, tolerance: 313.49797076941456\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "lasso.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, lasso.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso best parameters:{'fit__alpha': 0.0001}\n",
      "lasso best score-23.51478663084601\n",
      "train score-22.70277155343242\n",
      "validation score:-23.429495068080197\n",
      "test score:23.17512386871572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 736838.1482317887, tolerance: 313.49797076941456\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "lasso_regressor.fit(X2_train,y2_train)\n",
    "print(\"lasso best parameters:\"+ str(lasso_regressor.best_params_))\n",
    "print(\"lasso best score\" + str(lasso_regressor.best_score_))\n",
    "best_params = lasso_regressor.best_params_\n",
    "lasso = linear_model.Lasso(alpha=best_params['fit__alpha'])\n",
    "scores = cross_validate(lasso, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "lasso.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, lasso.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:32.84734762246127\n"
     ]
    }
   ],
   "source": [
    "#RFE\n",
    "clf_featr_sele = linear_model.Lasso(normalize = True)\n",
    "\n",
    "rfecv = RFECV(estimator=clf_featr_sele, step=1,scoring=\"neg_root_mean_squared_error\",cv=5)\n",
    "\n",
    "lasso_pipe = Pipeline([\n",
    "                 ('fit', linear_model.Lasso(normalize = True))]) \n",
    "lasso_regressor = GridSearchCV(lasso_pipe,lasso_params,scoring=\"neg_root_mean_squared_error\",cv=5)\n",
    "\n",
    "\n",
    "pipeline  = Pipeline([('feature_sele',rfecv),('clf_cv',lasso_regressor)])\n",
    "pipeline.fit(X_train, y_train)\n",
    "rmse= sqrt(mse(y_test, pipeline.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge best parameters:{'fit__alpha': 0.05}\n",
      "ridge best score-17.939723932583426\n"
     ]
    }
   ],
   "source": [
    "ridge_params = {'fit__alpha':[0.001,0.0015,0.01,0.05,0.1,0.5,1]\n",
    "}\n",
    "ridge_pipe= Pipeline([('poly', PolynomialFeatures()),\n",
    "                 ('fit', linear_model.Ridge(normalize = True))])\n",
    "\n",
    "ridge_regressor = GridSearchCV(ridge_pipe,ridge_params,scoring=\"neg_root_mean_squared_error\",cv=10)\n",
    "ridge_regressor.fit(X_train,y_train)\n",
    "\n",
    "print(\"ridge best parameters:\"+ str(ridge_regressor.best_params_))\n",
    "print(\"ridge best score\" + str(ridge_regressor.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-22.382747433035856\n",
      "validation score:-23.165863312065245\n"
     ]
    }
   ],
   "source": [
    "best_params = ridge_regressor.best_params_\n",
    "ridge = linear_model.Ridge(alpha=best_params['fit__alpha'])\n",
    "scores = cross_validate(ridge, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:23.147797707664022\n"
     ]
    }
   ],
   "source": [
    "ridge.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, ridge.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge best parameters:{'fit__alpha': 0.1}\n",
      "ridge best score-18.82699722078926\n",
      "train score-22.702771606712155\n",
      "validation score:-23.4294921620052\n",
      "test score:23.397125251883022\n"
     ]
    }
   ],
   "source": [
    "ridge_regressor.fit(X2_train,y2_train)\n",
    "\n",
    "print(\"ridge best parameters:\"+ str(ridge_regressor.best_params_))\n",
    "print(\"ridge best score\" + str(ridge_regressor.best_score_))\n",
    "best_params = ridge_regressor.best_params_\n",
    "ridge = linear_model.Ridge(alpha=best_params['fit__alpha'])\n",
    "scores = cross_validate(ridge, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "ridge.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, ridge.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=1.36013e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=1.38686e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=1.34588e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=7.10574e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=8.22607e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=5.20972e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=5.71527e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:188: LinAlgWarning: Ill-conditioned matrix (rcond=5.82046e-17): result may not be accurate.\n",
      "  overwrite_a=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit__alpha': 0.01, 'fit__gamma': 1e-05}\n",
      "-15.583061485987624\n"
     ]
    }
   ],
   "source": [
    "krr_params = {'fit__alpha':[1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10,100],\n",
    "              \n",
    "              'fit__gamma':[1e-15, 1e-10, 1e-8, 1e-5,1e-4, 1e-3,1e-2, 1, 5, 10,100],\n",
    "              }\n",
    "krr_pipe= Pipeline([('fit', KernelRidge(kernel='rbf'))])\n",
    "krr_regressor = GridSearchCV(krr_pipe,krr_params,scoring=\"neg_root_mean_squared_error\",cv=10)\n",
    "krr_regressor.fit(X_train,y_train)\n",
    "print(krr_regressor.best_params_)\n",
    "print(krr_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-12.084571441648182\n",
      "validation score:-15.583061485987622\n"
     ]
    }
   ],
   "source": [
    "best_params = krr_regressor.best_params_\n",
    "krr = KernelRidge(kernel='rbf',alpha=best_params['fit__alpha'],gamma = best_params['fit__gamma'])\n",
    "scores = cross_validate(krr, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:14.858413713406915\n"
     ]
    }
   ],
   "source": [
    "krr.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, krr.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_ridge.py:190: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.\n",
      "  warnings.warn(\"Singular matrix in solving dual problem. Using \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit__alpha': 0.001, 'fit__gamma': 0.01}\n",
      "-12.784905864614114\n",
      "train score-9.136656580809301\n",
      "validation score:-12.784905864614116\n",
      "test score:11.923829355588593\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "krr_regressor.fit(X2_train,y2_train)\n",
    "print(krr_regressor.best_params_)\n",
    "print(krr_regressor.best_score_)\n",
    "best_params = krr_regressor.best_params_\n",
    "krr = KernelRidge(kernel='rbf',alpha=best_params['fit__alpha'],gamma = best_params['fit__gamma'])\n",
    "scores = cross_validate(krr, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "krr.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, krr.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KRR best parameters:{'C': 1000, 'epsilon': 1, 'gamma': 0.0001}\n",
      "KRR best score-14.131693865652817\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "svr_params={\n",
    "            'C': [0.1, 1, 100, 1000],\n",
    "            'epsilon': [0.0001,0.001,0.01, 0.1, 1, 10],\n",
    "            'gamma': [0.0001, 0.001, 0.005, 0.1, 1, 3, 5]\n",
    "        }\n",
    "svr_regressor = GridSearchCV(SVR(kernel='rbf'),svr_params,scoring=\"neg_root_mean_squared_error\",cv=10)\n",
    "svr_regressor.fit(X_train,y_train)\n",
    "print(\"SVR best parameters:\"+ str(svr_regressor.best_params_))\n",
    "print(\"SVR best score\" + str(svr_regressor.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-8.764458588339803\n",
      "validation score:-14.131693865652817\n"
     ]
    }
   ],
   "source": [
    "best_params = svr_regressor.best_params_\n",
    "svr = SVR(C=best_params[\"C\"], epsilon=best_params[\"epsilon\"], gamma=best_params[\"gamma\"])\n",
    "scores = cross_validate(svr, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:13.365160616478182\n"
     ]
    }
   ],
   "source": [
    "svr.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, svr.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR best parameters:{'C': 100, 'epsilon': 0.0001, 'gamma': 1}\n",
      "SVR best score-11.556714655864395\n",
      "train score-5.278289105593005\n",
      "validation score:-11.556714655864395\n",
      "test score:11.24985386832459\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "svr_regressor.fit(X2_train,y2_train)\n",
    "print(\"SVR best parameters:\"+ str(svr_regressor.best_params_))\n",
    "print(\"SVR best score\" + str(svr_regressor.best_score_))\n",
    "best_params = svr_regressor.best_params_\n",
    "svr = SVR(C=best_params[\"C\"], epsilon=best_params[\"epsilon\"], gamma=best_params[\"gamma\"])\n",
    "scores = cross_validate(svr, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "svr.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, svr.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 100, 'max_leaf_nodes': 1500, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "rfr_params={\n",
    "            'max_depth': [10,20,30,40,50,100],\n",
    "            'max_leaf_nodes':[600,800,1000,1200,1500],\n",
    "            'n_estimators' :[10,30,40,50,60,80,100,150,200],\n",
    "        }\n",
    "\n",
    "rfr_regressor = GridSearchCV(\n",
    "        RandomForestRegressor(criterion=\"mse\",max_features=\"log2\"),\n",
    "        rfr_params,\n",
    "        cv=10, scoring=\"neg_root_mean_squared_error\", verbose=0, n_jobs=-1)\n",
    "rfr_regressor.fit(X_train,y_train)\n",
    "best_params = rfr_regressor.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-5.106095105498743\n",
      "validation score:-12.531301389851116\n"
     ]
    }
   ],
   "source": [
    "best_params = rfr_regressor.best_params_\n",
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"])\n",
    "scores = cross_validate(rfr, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:11.941898183692432\n"
     ]
    }
   ],
   "source": [
    "rfr.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, rfr.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 30, 'max_leaf_nodes': 1500, 'n_estimators': 200}\n",
      "train score-5.497762362403451\n",
      "validation score:-13.653597520933332\n",
      "test score:12.301780779511413\n"
     ]
    }
   ],
   "source": [
    "rfr_regressor.fit(X2_train,y2_train)\n",
    "best_params = rfr_regressor.best_params_\n",
    "print(best_params)\n",
    "best_params = rfr_regressor.best_params_\n",
    "rfr = RandomForestRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"])\n",
    "scores = cross_validate(rfr, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "rfr.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, rfr.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 50, 'max_leaf_nodes': 1500, 'n_estimators': 150}\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "etr_params={\n",
    "            'max_depth': [10,20,30,40,50,100],\n",
    "            'max_leaf_nodes':[600,800,1000,1200,1500],\n",
    "            'n_estimators' :[10,30,40,50,60,80,100,150,200],\n",
    "        }\n",
    "\n",
    "etr_regressor = GridSearchCV(\n",
    "       ExtraTreesRegressor(criterion=\"mse\",max_features=\"log2\"),\n",
    "        etr_params,\n",
    "        cv=10, scoring=\"neg_root_mean_squared_error\", verbose=0, n_jobs=-1)\n",
    "etr_regressor.fit(X_train,y_train)\n",
    "best_params = etr_regressor.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train score-2.5846190138088274\n",
      "validation score:-11.699920327246446\n"
     ]
    }
   ],
   "source": [
    "etr = ExtraTreesRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"])\n",
    "scores = cross_validate(etr, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test score:11.348520889949755\n"
     ]
    }
   ],
   "source": [
    "etr.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, etr.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 40, 'max_leaf_nodes': 1500, 'n_estimators': 80}\n",
      "train score-2.5846141925137394\n",
      "validation score:-11.992921293373803\n",
      "test score:11.001443069027072\n"
     ]
    }
   ],
   "source": [
    "etr_regressor.fit(X2_train,y2_train)\n",
    "best_params = etr_regressor.best_params_\n",
    "print(best_params)\n",
    "etr = ExtraTreesRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"])\n",
    "scores = cross_validate(etr, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "etr.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, etr.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:43:25] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "{'colsample': 0.8, 'learningrate': 0.05, 'max_depth': 4, 'n_estimators': 800, 'subsample': 1}\n"
     ]
    }
   ],
   "source": [
    "### Build model\n",
    "xgb_params={\n",
    "            'max_depth': [4,5,6,7,8],\n",
    "            'subsample': [0.8,0.9,1],\n",
    "            'colsample':[0.8,0.9,1],\n",
    "            'learningrate':[0.05],\n",
    "            'n_estimators' :[200,500,800],\n",
    "            \n",
    "        }\n",
    "\n",
    "xgb_regressor = GridSearchCV(\n",
    "       XGBRegressor(),\n",
    "        xgb_params,\n",
    "        cv=3, scoring=\"neg_root_mean_squared_error\", verbose=0, n_jobs=-1)\n",
    "xgb_regressor.fit(X_train,y_train)\n",
    "best_params = xgb_regressor.best_params_\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:43:28] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:31] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:34] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:37] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:40] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:44] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:47] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:50] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:53] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:43:56] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "train score-3.3924554833062865\n",
      "validation score:-10.142772485035389\n"
     ]
    }
   ],
   "source": [
    "best_params = xgb_regressor.best_params_\n",
    "xgb = XGBRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n",
    "                          subsample=best_params[\"subsample\"], colsample=best_params[\"colsample\"],\n",
    "                         learningrate=best_params[\"learningrate\"])\n",
    "scores = cross_validate(xgb, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:44:00] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "test score:10.254481960762751\n"
     ]
    }
   ],
   "source": [
    "xgb.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, xgb.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:51:37] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "{'colsample': 0.8, 'learningrate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 1}\n",
      "[11:51:43] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:51:49] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:51:55] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:01] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:07] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:13] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:20] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:26] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:32] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:52:38] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "train score-2.732527872427898\n",
      "validation score:-12.757858249557025\n",
      "[11:52:44] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "test score:12.267835996077181\n"
     ]
    }
   ],
   "source": [
    "#PCA\n",
    "xgb_regressor.fit(X2_train,y2_train)\n",
    "best_params = xgb_regressor.best_params_\n",
    "print(best_params)\n",
    "best_params = xgb_regressor.best_params_\n",
    "xgb = XGBRegressor(max_depth=best_params[\"max_depth\"], n_estimators=best_params[\"n_estimators\"],\n",
    "                          subsample=best_params[\"subsample\"], colsample=best_params[\"colsample\"],\n",
    "                         learningrate=best_params[\"learningrate\"])\n",
    "scores = cross_validate(xgb, X2_train, y2_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "xgb.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, xgb.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## XGBoost Feature importance\n",
    "from xgboost import plot_importance\n",
    "plot_importance(xgb, max_num_features=20) # top 20 most important features\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 666758.280161702, tolerance: 283.22311868342143\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:17] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 688239.9318200274, tolerance: 282.77879356326014\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:44:40] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 662734.9495226439, tolerance: 280.91643030579576\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:02] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 674876.0148734447, tolerance: 281.9867903623543\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:25] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 691352.4973018217, tolerance: 280.3100533675053\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:45:46] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 676410.9456473077, tolerance: 282.25500851243197\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:09] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 687675.7462200446, tolerance: 283.97788431827763\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:31] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 688254.3397479637, tolerance: 283.3227122371475\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:46:54] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675259.664109517, tolerance: 281.32803309270656\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:17] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 675523.0806616064, tolerance: 281.29787541161636\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:47:39] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "train score-9.371391899889499\n",
      "validation score:-12.903505522815458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/linear_model/_coordinate_descent.py:476: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 756769.3904974766, tolerance: 313.49797076941456\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:48:03] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "test score:12.430050535831718\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "voting_reg = VotingRegressor(\n",
    "    estimators=[('lasso',lasso),('ridge',ridge),('krr', krr),('svr',svr), ('rfr',rfr),('etr',rfr),('xgb', xgb)])\n",
    "scores = cross_validate(voting_reg, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "rmse= sqrt(mse(y_test, voting_reg.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:46:05] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:46:19] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:46:34] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:46:47] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:01] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:15] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:29] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:43] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:47:57] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:48:11] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "train score-4.189822343554723\n",
      "validation score:-11.048426101791689\n",
      "[10:48:26] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "test score:10.634649720415682\n"
     ]
    }
   ],
   "source": [
    "voting_reg2 = VotingRegressor(\n",
    "    estimators=[('rfr',rfr),('etr',rfr),('xgb', xgb)])\n",
    "scores = cross_validate(voting_reg2, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "voting_reg2.fit(X_train,y_train)\n",
    "rmse= sqrt(mse(y_test, voting_reg2.predict(X_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:51:53] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:52:07] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:52:21] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:52:35] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:52:48] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:53:02] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:53:16] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:53:30] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:53:44] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:53:58] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "train score-4.194234552877598\n",
      "validation score:-11.01716176948185\n",
      "[10:55:02] WARNING: /workspace/src/learner.cc:480: \n",
      "Parameters: { colsample, learningrate } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "test score:11.735597550020902\n"
     ]
    }
   ],
   "source": [
    "voting_reg2 = VotingRegressor(\n",
    "    estimators=[('rfr',rfr),('etr',rfr),('xgb', xgb)])\n",
    "scores = cross_validate(voting_reg2, X_train, y_train, cv=10,scoring='neg_root_mean_squared_error',return_train_score=True)\n",
    "print(\"train score\" + str(statistics.mean(scores['train_score'])))\n",
    "print(\"validation score:\" + str(statistics.mean(scores['test_score'])))\n",
    "voting_reg2.fit(X2_train,y2_train)\n",
    "rmse= sqrt(mse(y2_test, voting_reg2.predict(X2_test)))\n",
    "print(\"test score:\" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
